{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehULm8Ieu9B6"
      },
      "source": [
        "## Google Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pok6DhOgURJe",
        "outputId": "f75bf8dc-9d88-4abc-bff7-d12d2475c3bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m882.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.5.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m527.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17395 sha256=85a56cbbaaf022a66babbac5d808263a5219c0e22bca86877b902ba12171016b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.7\n",
            "    Uninstalling idna-3.7:\n",
            "      Successfully uninstalled idna-3.7\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.5.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade googletrans==4.0.0-rc1\n",
        "import pandas as pd\n",
        "from googletrans import Translator\n",
        "import asyncio\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncrDX-Fd5EXp",
        "outputId": "657372ff-1a21-4313-a859-84e92b85b55e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30/30 [00:05<00:00,  5.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indices with translation errors: []\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "<ipython-input-3-4677392ee3fb>:40: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_slice['eng_trans'] = df_slice.progress_apply(apply_translation, axis=1)\n",
            "<ipython-input-3-4677392ee3fb>:43: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_slice.dropna(subset=['eng_trans'], inplace=True)\n"
          ]
        }
      ],
      "source": [
        "import time  # Import the time module\n",
        "\n",
        "def translate_text(text, src_lang='hi', dest_lang='en'):\n",
        "    try:\n",
        "        translator = Translator()\n",
        "        translation = translator.translate(text, src=src_lang, dest=dest_lang)\n",
        "        return translation.text\n",
        "    except Exception as e:\n",
        "        print(f\"Translation error for text: {text}\")\n",
        "        return None\n",
        "\n",
        "# Assuming you have an Excel file with a sheet named 'Sheet1' and a column named 'hindi'\n",
        "# Replace 'your_dataset.xlsx' with the actual file path or Excel file\n",
        "df = pd.read_excel('/content/Sample_Data.xlsx')\n",
        "\n",
        "# Determine the total number of records in the DataFrame\n",
        "total_records = len(df)\n",
        "\n",
        "# Define the start and end indices for the slice\n",
        "start_index = 0\n",
        "end_index = 30 if total_records >= 2000 else total_records\n",
        "\n",
        "# Slice the DataFrame to get records 1001-2000\n",
        "df_slice = df.iloc[start_index:end_index]\n",
        "\n",
        "# Create a tqdm progress bar for the translation process\n",
        "tqdm.pandas()\n",
        "\n",
        "# Initialize a list to store indices where translation errors occur\n",
        "error_indices = []\n",
        "\n",
        "# Apply translation to each row in the 'hindi' column with tqdm\n",
        "def apply_translation(row):\n",
        "    translation = translate_text(row['hindi'])\n",
        "\n",
        "    if translation is None:\n",
        "        error_indices.append(row['Index'])\n",
        "    return translation\n",
        "\n",
        "df_slice['eng_trans'] = df_slice.progress_apply(apply_translation, axis=1)\n",
        "\n",
        "# Drop rows where translation failed (resulted in None)\n",
        "df_slice.dropna(subset=['eng_trans'], inplace=True)\n",
        "\n",
        "# Save the translated DataFrame slice to a new Excel file\n",
        "df_slice.to_excel('Data_with_Google_Trans.xlsx', index=False, sheet_name='Sheet1')\n",
        "\n",
        "# Print indices where translation errors occurred\n",
        "print(\"Indices with translation errors:\", error_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IztgyY72mxRd",
        "outputId": "b901aa2f-93bd-444e-992d-2b692b011b5e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               hindi  \\\n",
              "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
              "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
              "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
              "\n",
              "                                          english  \\\n",
              "0  Give your application an accessibility workout   \n",
              "1               Accerciser Accessibility Explorer   \n",
              "2  The default plugin layout for the bottom panel   \n",
              "3     The default plugin layout for the top panel   \n",
              "4  A list of plugins that are disabled by default   \n",
              "\n",
              "                                           eng_trans  \n",
              "0  Give the benefit of accessibility exercise to ...  \n",
              "1                Exercise accessibility investigator  \n",
              "2       Defolt plug-in blueprint for the lower board  \n",
              "3       Defolt plug in blueprint for the upper board  \n",
              "4                                             Plague  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-02eab28e-d3ff-44a6-b57e-a9d738ce092b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "      <th>eng_trans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
              "      <td>Give your application an accessibility workout</td>\n",
              "      <td>Give the benefit of accessibility exercise to ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
              "      <td>Accerciser Accessibility Explorer</td>\n",
              "      <td>Exercise accessibility investigator</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the bottom panel</td>\n",
              "      <td>Defolt plug-in blueprint for the lower board</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the top panel</td>\n",
              "      <td>Defolt plug in blueprint for the upper board</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
              "      <td>A list of plugins that are disabled by default</td>\n",
              "      <td>Plague</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-02eab28e-d3ff-44a6-b57e-a9d738ce092b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-02eab28e-d3ff-44a6-b57e-a9d738ce092b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-02eab28e-d3ff-44a6-b57e-a9d738ce092b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f94af455-8d20-4272-8aaf-2fa389e10ade\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f94af455-8d20-4272-8aaf-2fa389e10ade')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f94af455-8d20-4272-8aaf-2fa389e10ade button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_slice",
              "summary": "{\n  \"name\": \"df_slice\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"hindi\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"\\u0905\\u0902\\u0924\\u093f\\u092e \\u092a\\u094d\\u0930\\u0935\\u093f\\u0937\\u094d\\u091f \\u0918\\u091f\\u0928\\u093e \\u0915\\u094b \\u0939\\u093e\\u0907\\u0932\\u093e\\u0907\\u091f \\u0915\\u0930\\u094b\",\n          \"\\u092e\\u093e\\u0928\",\n          \"\\u0907\\u0938 \\u0938\\u092e\\u092f \\u091c\\u093f\\u0938\\u0947 \\u092a\\u094d\\u0930\\u093e\\u092a\\u094d\\u0924 \\u0915\\u093f\\u092f\\u093e \\u0917\\u092f\\u093e \\u0939\\u094b, \\u0909\\u0938\\u0915\\u0940 \\u0935\\u093f\\u092d\\u093f\\u0928\\u094d\\u0928 \\u0935\\u093f\\u0927\\u093f\\u092f\\u094b\\u0902 (\\u092e\\u0947\\u0925\\u0921) \\u092e\\u0947\\u0902 \\u0935\\u093f\\u091a\\u0930\\u0923 \\u0915\\u0930\\u0947\\u0902\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"english\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Shows events as they occur from selected types and sources\",\n          \"Property\",\n          \"Selected application\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_trans\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Highlight the final entered event\",\n          \"Value\",\n          \"Walk in various methods of what has been obtained at this time\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "df_slice.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwVvTp_-vE6B"
      },
      "source": [
        "## AI4 Bharath Translator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sf82-_FAjcuW",
        "outputId": "27a6382a-c194-4b45-ce39-1a86ff4d6518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n",
            "Cloning into 'indicTrans'...\n",
            "remote: Enumerating objects: 700, done.\u001b[K\n",
            "remote: Counting objects: 100% (403/403), done.\u001b[K\n",
            "remote: Compressing objects: 100% (161/161), done.\u001b[K\n",
            "remote: Total 700 (delta 280), reused 343 (delta 240), pack-reused 297\u001b[K\n",
            "Receiving objects: 100% (700/700), 2.64 MiB | 14.22 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n",
            "/content/indicTrans\n",
            "Cloning into 'indic_nlp_library'...\n",
            "remote: Enumerating objects: 1404, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 1404 (delta 139), reused 152 (delta 124), pack-reused 1219\u001b[K\n",
            "Receiving objects: 100% (1404/1404), 9.57 MiB | 12.69 MiB/s, done.\n",
            "Resolving deltas: 100% (749/749), done.\n",
            "Cloning into 'indic_nlp_resources'...\n",
            "remote: Enumerating objects: 139, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 139 (delta 2), reused 2 (delta 0), pack-reused 126\u001b[K\n",
            "Receiving objects: 100% (139/139), 149.77 MiB | 35.20 MiB/s, done.\n",
            "Resolving deltas: 100% (53/53), done.\n",
            "Updating files: 100% (28/28), done.\n",
            "Cloning into 'subword-nmt'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 600 (delta 10), reused 12 (delta 4), pack-reused 576\u001b[K\n",
            "Receiving objects: 100% (600/600), 253.58 KiB | 3.34 MiB/s, done.\n",
            "Resolving deltas: 100% (359/359), done.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Check if GPU is available\n",
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print(\"GPU is available\")\n",
        "    # Explicitly set a GPU device if multiple GPUs are available\n",
        "    # For example, use the first GPU (device: 0)\n",
        "    # You can change the device number as needed\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    if physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "\n",
        "# clone the repo for running evaluation\n",
        "!git clone https://github.com/AI4Bharat/indicTrans.git\n",
        "%cd indicTrans\n",
        "# clone requirements repositories\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_library.git\n",
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!git clone https://github.com/rsennrich/subword-nmt.git\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr-uO_iOjcm4",
        "outputId": "60794b53-64ef-4456-e0c8-cf2d58b8cfbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Collecting mock\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.2-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.12.25)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (24.0)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX) (3.20.3)\n",
            "Collecting sphinx-argparse (from indic-nlp-library)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library)\n",
            "  Downloading sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.21 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.8)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.6)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.10)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.0.7)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.1.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.14.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.16)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.5.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2024.2.2)\n",
            "Installing collected packages: morfessor, tensorboardX, sacremoses, portalocker, mock, colorama, sacrebleu, sphinxcontrib-jquery, sphinx-argparse, sphinx-rtd-theme, indic-nlp-library\n",
            "Successfully installed colorama-0.4.6 indic-nlp-library-0.92 mock-5.1.0 morfessor-2.0.6 portalocker-2.8.2 sacrebleu-2.4.2 sacremoses-0.1.1 sphinx-argparse-0.4.0 sphinx-rtd-theme-2.0.0 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2.2\n",
            "Collecting mosestokenizer\n",
            "  Downloading mosestokenizer-1.2.1.tar.gz (37 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting subword-nmt\n",
            "  Downloading subword_nmt-0.3.8-py3-none-any.whl (27 kB)\n",
            "Collecting docopt (from mosestokenizer)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting openfile (from mosestokenizer)\n",
            "  Downloading openfile-0.0.7-py3-none-any.whl (2.4 kB)\n",
            "Collecting uctools (from mosestokenizer)\n",
            "  Downloading uctools-1.3.0.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting toolwrapper (from mosestokenizer)\n",
            "  Downloading toolwrapper-2.1.0.tar.gz (3.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: mock in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (5.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from subword-nmt) (4.66.2)\n",
            "Building wheels for collected packages: mosestokenizer, docopt, toolwrapper, uctools\n",
            "  Building wheel for mosestokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mosestokenizer: filename=mosestokenizer-1.2.1-py3-none-any.whl size=49171 sha256=6bcadc7e2d645e6bed2a6e6c4aab090da76b6daaff5dd7cee062b77e71285042\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/d8/15/4c5ebbe883513f003cb055a0369c77c9df857023a706f39e70\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=182a1c28be682722c3a1f31b62d53a9cf6e19412d41f14a0a8c705145018f861\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for toolwrapper (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for toolwrapper: filename=toolwrapper-2.1.0-py3-none-any.whl size=3336 sha256=6d1a405dbad8eefd771a19ec095dd80c2e88ef200471e635e99eb7da39d8d3b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/af/b1/99b57a06dda78fdcee86d2e22c64743f3b8df8f31cfc04baf7\n",
            "  Building wheel for uctools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for uctools: filename=uctools-1.3.0-py3-none-any.whl size=6146 sha256=1dda185ea73da11b8865018233c1cfe838733e4d6e917b74f520f059ef7ce425\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/ee/10/33257b0801ac6a912c841939032c16da1eb3db377afe1443e5\n",
            "Successfully built mosestokenizer docopt toolwrapper uctools\n",
            "Installing collected packages: toolwrapper, openfile, docopt, uctools, subword-nmt, mosestokenizer\n",
            "Successfully installed docopt-0.6.2 mosestokenizer-1.2.1 openfile-0.0.7 subword-nmt-0.3.8 toolwrapper-2.1.0 uctools-1.3.0\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 35184, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (58/58), done.\u001b[K\n",
            "remote: Total 35184 (delta 61), reused 72 (delta 47), pack-reused 35079\u001b[K\n",
            "Receiving objects: 100% (35184/35184), 25.22 MiB | 26.16 MiB/s, done.\n",
            "Resolving deltas: 100% (25548/25548), done.\n",
            "/content/fairseq\n",
            "Processing /content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (3.0.10)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq==0.12.2)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq==0.12.2)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy>=1.21.3 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.25.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2023.12.25)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.4.2)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (4.66.2)\n",
            "Collecting bitarray (from fairseq==0.12.2)\n",
            "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (1.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fairseq==0.12.2) (24.0)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.11.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.14.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->fairseq==0.12.2) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13->fairseq==0.12.2)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq==0.12.2) (2.22)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fairseq==0.12.2) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->fairseq==0.12.2) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->fairseq==0.12.2) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=19929021 sha256=2ad64162d3a19b17e55adab1eafe98bf48d01c682f26efecc792e92bb6f93f3e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ymzpafez/wheels/c6/d7/db/bc419b1daa8266aa8de2a7c4d29f62dbfa814e8701fe4695a2\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141211 sha256=57d51d7248315a0c864758464ee1cbbf3f9eb3f70a289abb19d689e81e1ca571\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, omegaconf, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, nvidia-cusolver-cu12, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.9.2 fairseq-0.12.2 hydra-core-1.0.7 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 omegaconf-2.0.6\n",
            "Collecting xformers\n",
            "  Downloading xformers-0.0.26.post1-cp310-cp310-manylinux2014_x86_64.whl (222.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.25.2)\n",
            "Collecting torch==2.3.0 (from xformers)\n",
            "  Using cached torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.0.106)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0->xformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.0->xformers) (12.1.105)\n",
            "Collecting triton==2.3.0 (from torch==2.3.0->xformers)\n",
            "  Using cached triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->xformers) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.0->xformers) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.0->xformers) (1.3.0)\n",
            "Installing collected packages: triton, nvidia-nccl-cu12, torch, xformers\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.2.0\n",
            "    Uninstalling triton-2.2.0:\n",
            "      Successfully uninstalled triton-2.2.0\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.19.3\n",
            "    Uninstalling nvidia-nccl-cu12-2.19.3:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.19.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.2.1+cu121\n",
            "    Uninstalling torch-2.2.1+cu121:\n",
            "      Successfully uninstalled torch-2.2.1+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\n",
            "torchvision 0.17.1+cu121 requires torch==2.2.1, but you have torch 2.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-nccl-cu12-2.20.5 torch-2.3.0 triton-2.3.0 xformers-0.0.26.post1\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "! pip install mosestokenizer subword-nmt\n",
        "# Install fairseq from source\n",
        "!git clone https://github.com/pytorch/fairseq.git\n",
        "%cd fairseq\n",
        "# !git checkout da9eaba12d82b9bfc1442f0e2c6fc1b895f4d35d\n",
        "!pip install ./\n",
        "! pip install xformers\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ilN9gUGBjcdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6c59f99-8ec9-41a0-fdb2-c9f14dcc54e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:xformers:Triton is not available, some optimizations will not be enabled.\n",
            "This is just a warning: triton is not available\n"
          ]
        }
      ],
      "source": [
        "# add fairseq folder to python path\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "# sanity check to see if fairseq is installed\n",
        "from fairseq import checkpoint_utils, distributed_utils, options, tasks, utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PszIzStIjcBh",
        "outputId": "8982dcee-b5c8-427a-a9fc-3a3b36fbac97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-05-05 11:58:39--  https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
            "Resolving ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)... 164.52.210.96, 164.52.206.154, 164.52.210.97, ...\n",
            "Connecting to ai4b-public-nlu-nlg.objectstore.e2enetworks.net (ai4b-public-nlu-nlg.objectstore.e2enetworks.net)|164.52.210.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4759117228 (4.4G) [application/zip]\n",
            "Saving to: ‘indic2en.zip’\n",
            "\n",
            "indic2en.zip        100%[===================>]   4.43G  11.6MB/s    in 6m 43s  \n",
            "\n",
            "2024-05-05 12:05:26 (11.3 MB/s) - ‘indic2en.zip’ saved [4759117228/4759117228]\n",
            "\n",
            "Archive:  indic2en.zip\n",
            "   creating: indic-en/\n",
            "   creating: indic-en/vocab/\n",
            "  inflating: indic-en/vocab/bpe_codes.32k.SRC  \n",
            "  inflating: indic-en/vocab/vocab.SRC  \n",
            "  inflating: indic-en/vocab/vocab.TGT  \n",
            "  inflating: indic-en/vocab/bpe_codes.32k.TGT  \n",
            "   creating: indic-en/final_bin/\n",
            "  inflating: indic-en/final_bin/preprocess.log  \n",
            "  inflating: indic-en/final_bin/dict.TGT.txt  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/dict.SRC.txt  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.TGT.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.idx  \n",
            "  inflating: indic-en/final_bin/train.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/valid.SRC-TGT.SRC.bin  \n",
            "  inflating: indic-en/final_bin/test.SRC-TGT.SRC.bin  \n",
            "   creating: indic-en/model/\n",
            "  inflating: indic-en/model/checkpoint_best.pt  \n",
            "/content/indicTrans\n"
          ]
        }
      ],
      "source": [
        "# download the indictrans model\n",
        "# downloading the indic-en model\n",
        "!wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/indic2en.zip\n",
        "!unzip indic2en.zip\n",
        "\n",
        "# downloading the en-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/en2indic.zip\n",
        "# !unzip en2indic.zip\n",
        "\n",
        "# # downloading the indic-indic model\n",
        "# !wget https://ai4b-public-nlu-nlg.objectstore.e2enetworks.net/m2m.zip\n",
        "# !unzip m2m.zip\n",
        "%cd indicTrans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Y7U_gIVWjpwv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed6aadf0-87bf-46f4-f9a6-6fb1ae04bca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing vocab and bpe\n",
            "Initializing model for translation\n"
          ]
        }
      ],
      "source": [
        "from indicTrans.inference.engine import Model\n",
        "\n",
        "indic2en_model = Model(expdir='../indic-en')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fMa67NwnbD0e"
      },
      "outputs": [],
      "source": [
        "# Function to transliterate Hindi text to English\n",
        "def translate_to_english(text):\n",
        "    if pd.isnull(text) or not isinstance(text, str):\n",
        "        return ''  # or any default value\n",
        "    english_text = indic2en_model.translate_paragraph(text, 'hi', 'en')\n",
        "    return english_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VI9SLKg9gohz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "df = pd.read_excel('/content/Data_with_Google_Trans.xlsx')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FmC2yeoJHGMo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "52ea4982-ce72-4286-988e-f3bd2bad250d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/30 [00:00<?, ?it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1479.47it/s]\n",
            "  3%|▎         | 1/30 [00:02<01:08,  2.38s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00, 3855.06it/s]\n",
            "  7%|▋         | 2/30 [00:02<00:29,  1.06s/it]\n",
            "100%|██████████| 1/1 [00:00<00:00, 3994.58it/s]\n",
            " 10%|█         | 3/30 [00:02<00:17,  1.54it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4185.93it/s]\n",
            " 13%|█▎        | 4/30 [00:02<00:11,  2.22it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1754.94it/s]\n",
            " 17%|█▋        | 5/30 [00:02<00:08,  2.96it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 2307.10it/s]\n",
            " 20%|██        | 6/30 [00:03<00:06,  3.80it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 3705.22it/s]\n",
            " 23%|██▎       | 7/30 [00:03<00:05,  4.51it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1811.01it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 3342.07it/s]\n",
            " 30%|███       | 9/30 [00:03<00:03,  5.87it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 1787.85it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 3724.96it/s]\n",
            " 37%|███▋      | 11/30 [00:03<00:02,  6.81it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 2356.35it/s]\n",
            " 40%|████      | 12/30 [00:03<00:02,  7.21it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 2761.23it/s]\n",
            " 43%|████▎     | 13/30 [00:03<00:02,  6.83it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4258.18it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 5017.11it/s]\n",
            " 50%|█████     | 15/30 [00:04<00:01,  8.57it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 5761.41it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 5102.56it/s]\n",
            " 57%|█████▋    | 17/30 [00:04<00:01,  9.72it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 5343.06it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 3754.97it/s]\n",
            " 63%|██████▎   | 19/30 [00:04<00:01,  9.11it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4815.50it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 4064.25it/s]\n",
            " 70%|███████   | 21/30 [00:04<00:00, 10.32it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4306.27it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 5391.14it/s]\n",
            " 77%|███████▋  | 23/30 [00:04<00:00, 11.18it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4750.06it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 5275.85it/s]\n",
            " 83%|████████▎ | 25/30 [00:04<00:00, 12.24it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 5349.88it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 5282.50it/s]\n",
            " 90%|█████████ | 27/30 [00:05<00:00, 12.95it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 3284.50it/s]\n",
            "\n",
            "100%|██████████| 1/1 [00:00<00:00, 2392.64it/s]\n",
            " 97%|█████████▋| 29/30 [00:05<00:00, 10.70it/s]\n",
            "100%|██████████| 1/1 [00:00<00:00, 4279.90it/s]\n",
            "100%|██████████| 30/30 [00:05<00:00,  5.53it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               hindi  \\\n",
              "0    अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें   \n",
              "1                    एक्सेर्साइसर पहुंचनीयता अन्वेषक   \n",
              "2              निचले पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "3               ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका   \n",
              "4  उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...   \n",
              "\n",
              "                                          english  \\\n",
              "0  Give your application an accessibility workout   \n",
              "1               Accerciser Accessibility Explorer   \n",
              "2  The default plugin layout for the bottom panel   \n",
              "3     The default plugin layout for the top panel   \n",
              "4  A list of plugins that are disabled by default   \n",
              "\n",
              "                                           eng_trans  \\\n",
              "0  Give the benefit of accessibility exercise to ...   \n",
              "1                Exercise accessibility investigator   \n",
              "2       Defolt plug-in blueprint for the lower board   \n",
              "3       Defolt plug in blueprint for the upper board   \n",
              "4                                             Plague   \n",
              "\n",
              "                                         ai4_english  \n",
              "0  Give your application the benefit of an access...  \n",
              "1                  Accerciser Accessibility Explorer  \n",
              "2     The default plugin layout for the bottom panel  \n",
              "3        The default plugin layout for the top panel  \n",
              "4       List of plugins that are disabled by default  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c77a1df2-1549-4d5d-a7e5-24e216e55f65\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hindi</th>\n",
              "      <th>english</th>\n",
              "      <th>eng_trans</th>\n",
              "      <th>ai4_english</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>अपने अनुप्रयोग को पहुंचनीयता व्यायाम का लाभ दें</td>\n",
              "      <td>Give your application an accessibility workout</td>\n",
              "      <td>Give the benefit of accessibility exercise to ...</td>\n",
              "      <td>Give your application the benefit of an access...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>एक्सेर्साइसर पहुंचनीयता अन्वेषक</td>\n",
              "      <td>Accerciser Accessibility Explorer</td>\n",
              "      <td>Exercise accessibility investigator</td>\n",
              "      <td>Accerciser Accessibility Explorer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>निचले पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the bottom panel</td>\n",
              "      <td>Defolt plug-in blueprint for the lower board</td>\n",
              "      <td>The default plugin layout for the bottom panel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ऊपरी पटल के लिए डिफोल्ट प्लग-इन खाका</td>\n",
              "      <td>The default plugin layout for the top panel</td>\n",
              "      <td>Defolt plug in blueprint for the upper board</td>\n",
              "      <td>The default plugin layout for the top panel</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>उन प्लग-इनों की सूची जिन्हें डिफोल्ट रूप से नि...</td>\n",
              "      <td>A list of plugins that are disabled by default</td>\n",
              "      <td>Plague</td>\n",
              "      <td>List of plugins that are disabled by default</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c77a1df2-1549-4d5d-a7e5-24e216e55f65')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c77a1df2-1549-4d5d-a7e5-24e216e55f65 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c77a1df2-1549-4d5d-a7e5-24e216e55f65');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-54d99eff-6c3b-41e3-a996-3579bb76c13e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-54d99eff-6c3b-41e3-a996-3579bb76c13e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-54d99eff-6c3b-41e3-a996-3579bb76c13e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30,\n  \"fields\": [\n    {\n      \"column\": \"hindi\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"\\u0905\\u0902\\u0924\\u093f\\u092e \\u092a\\u094d\\u0930\\u0935\\u093f\\u0937\\u094d\\u091f \\u0918\\u091f\\u0928\\u093e \\u0915\\u094b \\u0939\\u093e\\u0907\\u0932\\u093e\\u0907\\u091f \\u0915\\u0930\\u094b\",\n          \"\\u092e\\u093e\\u0928\",\n          \"\\u0907\\u0938 \\u0938\\u092e\\u092f \\u091c\\u093f\\u0938\\u0947 \\u092a\\u094d\\u0930\\u093e\\u092a\\u094d\\u0924 \\u0915\\u093f\\u092f\\u093e \\u0917\\u092f\\u093e \\u0939\\u094b, \\u0909\\u0938\\u0915\\u0940 \\u0935\\u093f\\u092d\\u093f\\u0928\\u094d\\u0928 \\u0935\\u093f\\u0927\\u093f\\u092f\\u094b\\u0902 (\\u092e\\u0947\\u0925\\u0921) \\u092e\\u0947\\u0902 \\u0935\\u093f\\u091a\\u0930\\u0923 \\u0915\\u0930\\u0947\\u0902\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"english\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30,\n        \"samples\": [\n          \"Shows events as they occur from selected types and sources\",\n          \"Property\",\n          \"Selected application\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eng_trans\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Highlight the final entered event\",\n          \"Value\",\n          \"Walk in various methods of what has been obtained at this time\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ai4_english\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29,\n        \"samples\": [\n          \"Highlight the last entered event\",\n          \"value (p)\",\n          \"Walk in the different methods of what is currently achieved\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "df['ai4_english']=None\n",
        "for n, text in enumerate(tqdm(df['hindi'])):\n",
        "  df['ai4_english'][n] = translate_to_english(text)\n",
        "  # break\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "D_mcKOQxHIXo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "2cbe1a81-9322-4521-f037-a4945df8bcde"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_bbeb3f51-c74a-45cb-8bcf-b14210352f64\", \"Translated_Data_with_GoogleTrans_&_AI4Bharat.xlsx\", 7458)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df.to_excel('Translated_Data_with_GoogleTrans_&_AI4Bharat.xlsx')\n",
        "from google.colab import files\n",
        "files.download('Translated_Data_with_GoogleTrans_&_AI4Bharat.xlsx')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STS Score"
      ],
      "metadata": {
        "id": "VJFmiiQrmMm3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iK1PL2Mlgg-U",
        "outputId": "255a7ee3-c437-426f-dc01-6345ebaa6385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "100%|██████████| 30/30 [00:01<00:00, 22.55it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 4514.70it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 276.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed. File 'Score_Google_Trans_text.xlsx' saved.\n",
            "Overall Score: 0.46291333333333334\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = tokenizer.tokenize(str(text))\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    preprocessed_sentence1 = preprocess(sentence1)\n",
        "    preprocessed_sentence2 = preprocess(sentence2)\n",
        "\n",
        "    if not preprocessed_sentence1 or not preprocessed_sentence2:\n",
        "        return 0.0  # Return 0 similarity for empty vocabulary\n",
        "\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vectors = vectorizer.fit_transform([preprocessed_sentence1, preprocessed_sentence2])\n",
        "\n",
        "        similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "        return similarity if not (similarity != similarity) else 0.0  # Check for NaN similarity\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing record: {e}\")\n",
        "        return None  # Return None for problematic records\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with 'english' and 'eng_trans' columns\n",
        "# If not, replace 'df' with your actual DataFrame\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('Translated_Data_with_GoogleTrans_&_AI4Bharat.xlsx')  # Replace 'your_dataset.csv' with your actual file path\n",
        "\n",
        "# Use tqdm for progress tracking\n",
        "tqdm.pandas()\n",
        "\n",
        "# Apply preprocessing and calculate similarity scores\n",
        "df['preprocessed_english'] = df['english'].progress_apply(preprocess)\n",
        "df['preprocessed_eng_trans'] = df['eng_trans'].progress_apply(preprocess)\n",
        "df['sts_score_Google_Trans_text'] = df.progress_apply(lambda row: sentence_similarity(row['preprocessed_english'], row['preprocessed_eng_trans']), axis=1)\n",
        "\n",
        "# Drop rows with None values (indicating errors during processing)\n",
        "df.dropna(subset=['sts_score_Google_Trans_text'], inplace=True)\n",
        "\n",
        "# Round the STS scores to 4 decimals\n",
        "df['sts_score_Google_Trans_text'] = df['sts_score_Google_Trans_text'].round(4)\n",
        "\n",
        "# Save the updated dataset to a new Excel file\n",
        "df.to_excel('Final_Score_Google_Trans_text.xlsx', index=False)\n",
        "print(\"Processing completed. File 'Score_Google_Trans_text.xlsx' saved.\")\n",
        "\n",
        "# Calculate the overall score\n",
        "overall_score = df['sts_score_Google_Trans_text'].mean()\n",
        "\n",
        "print(f\"Overall Score: {overall_score}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = tokenizer.tokenize(str(text))\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def sentence_similarity(sentence1, sentence2):\n",
        "    preprocessed_sentence1 = preprocess(sentence1)\n",
        "    preprocessed_sentence2 = preprocess(sentence2)\n",
        "\n",
        "    if not preprocessed_sentence1 or not preprocessed_sentence2:\n",
        "        return 0.0  # Return 0 similarity for empty vocabulary\n",
        "\n",
        "    try:\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        vectors = vectorizer.fit_transform([preprocessed_sentence1, preprocessed_sentence2])\n",
        "\n",
        "        similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
        "        return similarity if not (similarity != similarity) else 0.0  # Check for NaN similarity\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing record: {e}\")\n",
        "        return None  # Return None for problematic records\n",
        "\n",
        "# Assuming you have a DataFrame named 'df' with 'english' and 'eng_trans' columns\n",
        "# If not, replace 'df' with your actual DataFrame\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('Final_Score_Google_Trans_text.xlsx')  # Replace 'your_dataset.csv' with your actual file path\n",
        "\n",
        "# Use tqdm for progress tracking\n",
        "tqdm.pandas()\n",
        "\n",
        "# Apply preprocessing and calculate similarity scores\n",
        "df['preprocessed_english'] = df['english'].progress_apply(preprocess)\n",
        "df['preprocessed_ai4_english'] = df['ai4_english'].progress_apply(preprocess)\n",
        "df['sts_score_Ai4Bharat_text'] = df.progress_apply(lambda row: sentence_similarity(row['preprocessed_english'], row['preprocessed_ai4_english']), axis=1)\n",
        "\n",
        "# Drop rows with None values (indicating errors during processing)\n",
        "df.dropna(subset=['sts_score_Ai4Bharat_text'], inplace=True)\n",
        "\n",
        "# Round the STS scores to 4 decimals\n",
        "df['sts_score_Ai4Bharat_text'] = df['sts_score_Ai4Bharat_text'].round(4)\n",
        "\n",
        "# Save the updated dataset to a new Excel file\n",
        "df.to_excel('Final(Score)_Translated(Both GT and AI)_Data.xlsx', index=False)\n",
        "print(\"Processing completed. File 'Final(Score)_Translated(Both GT and AI)_Data.xlsx' saved.\")\n",
        "\n",
        "# Calculate the overall score\n",
        "overall_score = df['sts_score_Ai4Bharat_text'].mean()\n",
        "\n",
        "print(f\"Overall Score: {overall_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igxSuIB2iA-r",
        "outputId": "e47cfd17-75be-49ea-f705-2333bf262944"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "100%|██████████| 30/30 [00:00<00:00, 4195.98it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 3910.89it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 361.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed. File 'Final(Score)_Translated(Both GT and AI)_Data.xlsx' saved.\n",
            "Overall Score: 0.6747666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARJyy98sYXC-",
        "outputId": "38eee016-e13f-4e39-f0fd-7f1bd1d52fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google_Trans Better: 6\n",
            "Ai4bharat Trans Better: 16\n",
            "--------------------------------------------\n",
            "Both Are Equal: 8\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_data.csv' with the path to your CSV file\n",
        "file_path = 'Final(Score)_Translated(Both GT and AI)_Data.xlsx'\n",
        "\n",
        "# Load data from CSV file into a pandas DataFrame\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Count occurrences where sts_score_Google_Trans_text > sts_score_ai4_english_text\n",
        "greater_than_count = (df['sts_score_Google_Trans_text'] > df['sts_score_Ai4Bharat_text']).sum()\n",
        "\n",
        "# Count occurrences where sts_score_Google_Trans_text < sts_score_ai4_english_text\n",
        "less_than_count = (df['sts_score_Google_Trans_text'] < df['sts_score_Ai4Bharat_text']).sum()\n",
        "\n",
        "# Count occurrences where sts_score_Google_Trans_text == sts_score_ai4_english_text\n",
        "equal_to_count = (df['sts_score_Google_Trans_text'] == df['sts_score_Ai4Bharat_text']).sum()\n",
        "\n",
        "print(f\"Google_Trans Better: {greater_than_count}\")\n",
        "print(f\"Ai4bharat Trans Better: {less_than_count}\")\n",
        "print(\"--------------------------------------------\")\n",
        "print(f\"Both Are Equal: {equal_to_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xRpcZqmcYp7h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb87cc0f-9610-4ab6-96fc-2bda94255093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "100%|██████████| 30/30 [00:00<00:00, 4907.53it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 2885.53it/s]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 30/30 [00:00<00:00, 3380.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed. File 'Final_BLEU_Score.xlsx' saved.\n",
            "Overall BLEU Score: 0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = tokenizer.tokenize(str(text))\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    return sentence_bleu([reference_tokens], candidate_tokens)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('Final(Score)_Translated(Both GT and AI)_Data.xlsx')  # Replace 'your_dataset.csv' with your actual file path\n",
        "\n",
        "# Use tqdm for progress tracking\n",
        "tqdm.pandas()\n",
        "\n",
        "# Apply preprocessing and calculate BLEU scores\n",
        "df['preprocessed_english'] = df['english'].progress_apply(preprocess)\n",
        "df['preprocessed_eng_trans'] = df['eng_trans'].progress_apply(preprocess)\n",
        "df['bleu_score'] = df.progress_apply(lambda row: calculate_bleu(row['preprocessed_english'], row['preprocessed_eng_trans']), axis=1)\n",
        "\n",
        "# Round the BLEU scores to 4 decimals\n",
        "df['bleu_score'] = df['bleu_score'].round(4)\n",
        "\n",
        "# Save the updated dataset to a new Excel file\n",
        "df.to_excel('Final_BLEU_Score_GT.xlsx', index=False)\n",
        "print(\"Processing completed. File 'Final_BLEU_Score.xlsx' saved.\")\n",
        "\n",
        "# Calculate the overall BLEU score\n",
        "overall_bleu_score = df['bleu_score'].mean()\n",
        "print(f\"Overall BLEU Score: {overall_bleu_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess(text):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    words = tokenizer.tokenize(str(text))\n",
        "    words = [lemmatizer.lemmatize(word.lower()) for word in words if word.isalpha()]\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def calculate_bleu(reference, candidate):\n",
        "    reference_tokens = reference.split()\n",
        "    candidate_tokens = candidate.split()\n",
        "\n",
        "    return sentence_bleu([reference_tokens], candidate_tokens)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_excel('Final_BLEU_Score_GT.xlsx')\n",
        "\n",
        "# Use tqdm for progress tracking\n",
        "tqdm.pandas()\n",
        "\n",
        "# Apply preprocessing and calculate BLEU scores\n",
        "df['preprocessed_english'] = df['english'].progress_apply(preprocess)\n",
        "df['preprocessed_ai4_english'] = df['ai4_english'].progress_apply(preprocess)\n",
        "df['bleu_score_ai4'] = df.progress_apply(lambda row: calculate_bleu(row['preprocessed_english'], row['preprocessed_ai4_english']), axis=1)\n",
        "\n",
        "# Save the updated dataset to a new Excel file\n",
        "df.to_excel('Final(Score)_Translated(Both GT and AI)_Data_BLEU.xlsx', index=False)\n",
        "print(\"Processing completed. File 'Final(Score)_Translated(Both GT and AI)_Data_BLEU.xlsx' saved.\")\n",
        "\n",
        "# Calculate the overall BLEU score\n",
        "overall_bleu_score = df['bleu_score_ai4'].mean()\n",
        "print(f\"Overall BLEU Score: {overall_bleu_score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bjw9g3YauyZ5",
        "outputId": "defa5042-a40f-45a4-d595-8c945dd433a1"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "100%|██████████| 30/30 [00:00<00:00, 4410.57it/s]\n",
            "100%|██████████| 30/30 [00:00<00:00, 3971.00it/s]\n",
            "  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "100%|██████████| 30/30 [00:00<00:00, 7537.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing completed. File 'Final(Score)_Translated(Both GT and AI)_Data_BLEU.xlsx' saved.\n",
            "Overall BLEU Score: 0.16666666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Replace 'your_data.csv' with the path to your CSV file\n",
        "file_path = 'Final(Score)_Translated(Both GT and AI)_Data_BLEU.xlsx'\n",
        "\n",
        "# Load data from CSV file into a pandas DataFrame\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Count occurrences where sts_score_Google_Trans_text > sts_score_ai4_english_text\n",
        "greater_than_count = (df['bleu_score'] > df['bleu_score_ai4']).sum()\n",
        "\n",
        "# Count occurrences where sts_score_Google_Trans_text < sts_score_ai4_english_text\n",
        "less_than_count = (df['bleu_score'] < df['bleu_score_ai4']).sum()\n",
        "\n",
        "# Count occurrences where sts_score_Google_Trans_text == sts_score_ai4_english_text\n",
        "equal_to_count = (df['bleu_score'] == df['bleu_score_ai4']).sum()\n",
        "\n",
        "print(f\"Google_Trans Better: {greater_than_count}\")\n",
        "print(f\"Ai4bharat Trans Better: {less_than_count}\")\n",
        "print(\"--------------------------------------------\")\n",
        "print(f\"Both Are Equal: {equal_to_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRb49zgvB4it",
        "outputId": "b67a9cc8-7705-4ae8-d91e-01f93f913122"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google_Trans Better: 0\n",
            "Ai4bharat Trans Better: 27\n",
            "--------------------------------------------\n",
            "Both Are Equal: 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qlzCc9_dCIYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "ehULm8Ieu9B6"
      ],
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}